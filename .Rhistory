fit3<-glm(t ~ shuttle$wind,family='binomial')
summary(fit)
summary(fit3)
?InsectSprays
fit4<-glm(InsectSprays$count~as.factor(InsectSprays$spray),family='poisson')
fit4$coefficients
fit4<-glm(InsectSprays$count~1-as.factor(InsectSprays$spray),family='poisson')
fit4$coefficients
fit4<-glm(InsectSprays$count~as.factor(InsectSprays$spray)-1,family='poisson')
fit4$coefficients
fit4$coefficients[1]/fit4$coefficients[2]
x <- -5:5
y <- c(5.12, 3.93, 2.67, 1.87, 0.52, 0.08, 0.93, 2.05, 2.54, 3.87, 4.97)
plot(x,y)
fit5<-lm(y~x)
abline(fit5)
fit<-glm(t~shuttle$wind,family='binomial')
exp(fit$coefficients)
fit<-glm(t~shuttle$wind-1,family='binomial')
exp(fit$coefficients)
exp(fit$coefficients)[1]/exp(fit$coefficients)[2]
fit<-glm(t~shuttle$wind+shuttle$magn-1,family='binomial')
exp(fit$coefficients)[1]/exp(fit$coefficients)[2]
exp(fit$coefficients)
fit<-glm(1-t~shuttle$wind+shuttle$magn-1,family='binomial')
summary(fit)
fit<-glm(t~shuttle$wind+shuttle$magn-1,family='binomial')
summary(fit)
source("http://bioconductor.org/biocLite.R")
biocLite("DESeq")
library(nlme)
library(lattice)
xyplot(weight ~ Time | Diet, BodyWeight)
library(nlme)
library(lattice)
xyplot(weight ~ Time | Diet)
library(nlme)
library(lattice)
xyplot(weight ~ Time | BodyWeight)
xyplot(weight ~ Time | Diet, BodyWeight)
BodyWeight
str(BodyWeight)
?lines()
?lpoints()
library(caret)
install.packages('caret')
library(caret)
install.packages('ggplot2')
install.packages("ggplot2")
library(caret)
library(AppliedPredictiveModeling)
library(AppliedPredictiveModeling)
install.packages('library(AppliedPredictiveModeling)')
install.packages('AppliedPredictiveModeling')
library(AppliedPredictiveModeling)
str(AppliedPredictiveModeling::)
str(AppliedPredictiveModeling)
data(AlzheimerDisease)
diagnosis
?diagnosis
?createDataPartition
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
?cut2
install.packages('Hmisc')
library(Hmisc)
?cut2
testing
head(testing)
plot(testing$CompressiveStrength ~ inTrain )
Summary(testing)
summary(testing)
plot(testing$CompressiveStrength ~ testing$FlyAsh)
?cut2
plot(testing$CompressiveStrength ~ testing$Age)
?featurePlot
cut2(testing$Age)
testing$Age
cut2(testing$Age)
cut2(testing$Age,2)
cut2(testing$Age)
cut2(testing$Age,10)
plot(testing$CompressiveStrength ~ cut2(testing$Age,28))
plot(testing$CompressiveStrength ~ cut2(testing$FlyAsh,0.1))
testing$FlyAsh
mean(testing$FlyAsh)
cut2(testing$FlyAsh,mean(testing$FlyAsh))
plot(testing$CompressiveStrength~cut2(testing$FlyAsh,mean(testing$FlyAsh)))
length(traing)
length(training)
length(testing)
dim(traingn)
dim(training
)
dim(testing)
mean(training$Age)
mean(training$FlyAsh)
plot(training$CompressiveStrength~cut2(training$FlyAsh,mean(training$FlyAsh)))
plot(training$CompressiveStrength~cut2(training$Age,mean(training$Age)))
inTrain
-inTrain
plot(training$CompressiveStrength ~ inTrain)
?cut2
plot(training$CompressiveStrength ~ cut2(inTrain,g=4))
featurePlot(x=cut2(training[,'FlyAsh'],g=2),y=training$CompressiveStrength,plot='pairs')
training$Superplasticizer
training$Superplasticizer<0
sum(training$Superplasticizer<0)
preProc<-preProcess(training,method='pca',pcaComp=2)
trainPC<-predict(preProc,training)
training$Superplasticizer
hist(training$Superplasticizer)
hist(log2(training$Superplasticizer)
)
log(training$Superplasticizer +1)
log(training$Superplasticizer)
log(training$Superplasticizer+1)
hist(log(training$Superplasticizer+1))
source("http://bioconductor.org/biocLite.R")
biocLite()
biocLite(edgeR)
biocLite('edgeR')
library(edgeR)
biocLite('limma')
biocLite('edgeR')
library(edgeR)
biocLite('ebayseq')
biocLite('bayseq')
2
biocLite('baySeq')
library(knitr)
---
title: "Regression model Project"
data(mtcars)
str(mtcars)
mtcars$am
mtcars$am<-as.factor(mtcars$am)
str(mtcars)
factor(mtcars$am)
level(mtcars$am)<-c('manual','auto')
levels(mtcars$am)<-c('manual','auto')
mtcars$am
source('~/.active-rstudio-document', echo=TRUE)
library(knitr)
library(maps)
install.packages('maps')
?by
by(mtcars$mpg,mtcars$am,t.test)
t.test(mtcars$mpg,mtcars$am)
by(mtcars$mpg,mtcars$am,t.test,simplify=0)
t.test(mtcars$mpg[mpg$am=='auto'],mtcars$mpg[mpg$am='manual'])
t.test(mtcars$mpg[mpg$am=='auto'],mtcars$mpg[mpg$am=='manual'])
mtcars$mpg[mpg$am=='auto']
t.test(mtcars$mpg[mtcars$am=='auto'],mtcars$mpg[mtcars$am='manual'])
t.test(mtcars$mpg[mtcars$am=='auto'],mtcars$mpg[mtcars$am=='manual'])
t<-t.test(mtcars$mpg[mtcars$am=='auto'],mtcars$mpg[mtcars$am=='manual'])
t$p.value
ttest<-t.test(mtcars$mpg[mtcars$am=='auto'],mtcars$mpg[mtcars$am=='manual'])
ttest
summary(mt$mpg)
summary(mtcars$mpg)
?summary
t<-summary(mtcars$mpg)
t[3]
t[4]
summary(mtcars$mpg[mtcars$am=='auto'])
summary(mtcars$mpg[mtcars$am=='auto'])-summary(mtcars$mpg[mtcars$am=='manual'])
?mtcars
auto_vs_weight<-lm(mtcars$mpg~mtcars$wt)
auto_vs_weight$coefficient
manual_vs_weight$coefficient
ggplot(mtcars,aes(am,colour=am))+geom_barplot()+facet_wrap(~cyl)
ggplot(mtcars,aes(am,mpg,colour=am))+geom_boxplot()+facet_wrap(~cyl)
ggplot(mtcars,aes(cyl,mpg,colour=am))+geom_point()
?mtcars
manual_vs_weight<-lm(mtcars$mpg[mtcars$am=='manual']~mtcars$wt[mtcars$am=='manual'])
plot(manual_vs_weight)
plot(mtcars$wt[mtcars$am=='auto'],resid(auto_vs_weight))
resid(auto_vs_weight)
mtcars$wt[mtcars$am=='auto']
auto_vs_weight<-lm(mtcars$mpg[mtcars$am=='auto']~mtcars$wt[mtcars$am=='auto'])
plot(mtcars$wt[mtcars$am=='auto'],resid(auto_vs_weight))
resid(auto_vs_weight)
library(caret)
library(ggplot2)
library(ISLR)
data(Wage)
install.packages('ISLR')
library(ISLR)
data(Wage)
?Wage
head(Wage)
names(Wage)
head(row.names(Wage))
?createDataPartition
inTrain<-createDataPartition(y=Wage$wage,p=0.7,list=FALSE)
training<-Wage[inTrain,]
testing<-Wage[-inTrain,]
dim(training)
dim(testing)
featurePlot(x=training[,c('age','education')],y=training$wage,plot='pairs')
?featurepl
?featurePlot
?iris
?train
names(getModelInfo())
library(rattle)
install.packages('rattle')
training<-read.csv('pml-training.csv')
testing<-read.csv('pml-testing.csv')
dim(training)
names(training)
head(training,1)
dim(testing)
library(kernlab)
names(training)
is.na(training)
sapply(training,function(x) sum(is.na(x)))
str(training$X)
names(training)
?grep
grep('.*dumbbell.*',names(training))
names(training)[grep('.*dumbbell.*',names(training))]
mymodel<-train(roll_dumbbell,method='rpart',data=training)
mymodel<-train(roll_dumbbell~,method='rpart',data=training)
mymodel<-train(roll_dumbbell ~ .,method='rpart',data=training)
mymodel<-train(classe ~ grep('.*dumbbell.*',names(training)),method='rpart',data=training)
dumbbell_training<-training[,c(grep('.*dumbbell.*',names(training)),'classe')]
dumbbell_training<-training[,c(grep('.*dumbbell.*',names(training)),'classe')]
dummbell_num<-grep('.*dumbbell.*',names(training))
dumbell_num
dummbell_num<-c(dummbell_num,160)
dummbell_num
mymodel<-(classe~.,method='glm',data=training)
mymodel<-(classe ~ .,method='glm',data=training)
mymodel<-(classe ~ ., method='glm',data=training)
mymodel<-(classe ~ . ,method='glm',data=training)
mymodel<-train(classe ~ . ,method='glm',data=training)
mymodel<-train(classe ~ . ,method='lm',data=training)
mymodel<-train(classe ~ . ,method='rpart',data=training)
training<-read.csv('pml-training.csv')
testing<-read,csv('pml-testing.csv')
testing<-read.csv('pml-testing.csv')
library(caret)
Modelfit<-train(classe~.,method='lm',data=training)
names(training)
Modelfit<-train(classe~.-user_name-X,method='lm',data=training)
Modelfit<-train(classe~.-user_name-X,method='rpart',data=training)
install.packages('e1071')
Modelfit<-train(classe~.-user_name-X,method='rpart',data=training)
plot(Modelfit$finalModel,uniform = TRUE)
predict_data<-predict(Modelfit,newdata=testing)
warning
names(training)
?corre
sapply(training,function(x) cor(x,training$classe))
training$classe
Modelfit
predict(Modelfit,newdata = testing)
sapply(training,function(x) cor(x,as.numeric(training$classe)))
sapply(training,function(x) cor(as.numeric(x),as.numeric(training$classe)))
allcorr<-sapply(training,function(x) cor(as.numeric(x),as.numeric(training$classe)))
allcorr>0.3
sum(allcorr>0.3)
sum(allcorr>0.3,rm.na=1)
sum(allcorr>0.3,na.rm = 1)
good_cor<-allcorr>0.3
allcorr<-0.3
allcorr<-sapply(training,function(x) cor(as.numeric(x),as.numeric(training$classe)))
allcorr<(-0.3)
sum(allcorr<(-0.3),na.rm = 1)
names(training)[good_cor]
good_cor
!is.na(good_cor)
good_cor
Modelfit<-train(classe~pitch_forearm,method='lm',data=training)
Modelfit<-train(as.numeric(classe)~pitch_forearm,method='lm',data=training)
prediction<-predict(Modelfit,newdata = testing)
prediction
confusionMatrix(prediction,training$classe)
confusionMatrix(prediction,as.numerictraining$classe)
confusionMatrix(prediction,as.numeric(training$classe))
confusionMatrix(prediction,as.numeric(testing$classe))
confusionMatrix(prediction,testing$classe)
Modelfit<-train(classe~pitch_forearm,method='rpart',data=training)
prediction<-predict(Modelfit,newdata = testing)
confusionMatrix(prediction,testing$classe)
prediction
testing$classe
names(test)
names(testing)
names(training)[good_cor]
names(training)[good_cor][1]
names(training)[good_cor][2]
names(training)[!is.na(names(training)[good_cor])]
set.seed(12345)
dim(testing)
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
pml_write_files(prediction)
prediction
source("http://bioconductor.org/biocLite.R")
biocLite("BitSeq")
biocLite("DEGseq")
getwd()
read.csv('getdata-data-ss06hid.csv')
t<-read.csv('getdata-data-ss06hid.csv')
names(t)
strplit(names(t))
strsplit(names(t))
strsplit(names(t)split = 'wgtp')
strsplit(names(t),split = 'wgtp')
s<-read.csv('getdata-data-GDP.csv')
s$GDP
gsub(levels(s$GDP))
gsub(levels(s$GDP),',')
?gsub
gsub(levels(s$GDP),',','')
gsub(',','',levels(s$GDP),)
levels(s$GDP)<-gsub(',','',levels(s$GDP))
mean(as.numeric(as.character(s$GDP)))
names(s)
s$Country
s$Economy
as.character(s$Economy)
grep("^United",as.character(s$Economy))
as.character(s$Economy)
country_name<-as.character(s$Economy)
grep("^United*",country_name
)
country_name[99]
u<-read.csv('getdata-data-EDSTATS_Country.csv')
head(u)
merge(u,s,by.x='CountryCode',by.y=Country)
merge(u,s,by.x='CountryCode',by.y='Country')
merged<-merge(u,s,by.x='CountryCode',by.y='Country')
names(merged)
head(merged)
merged$Special.Notes
as.character(merged$Special.Notes)
special_notes<-as.character(merged$Special.Notes)
grep("^Fiscal year end",special_notes)
special_notes[grep("^Fiscal year end",special_notes)]
special_notes<-special_notes[grep("^Fiscal year end",special_notes)]
strsplit('Fiscal year end: ',special_notes)
strsplit(special_notes,'Fiscal year end: ')
sapply(strsplit(special_notes,'Fiscal year end: '),'[[')[2]
sapply(strsplit(special_notes,'Fiscal year end: '),'[['[2])
sapply(strsplit(special_notes,'Fiscal year end: '),'[[')
?'[['
sapply(strsplit(special_notes,'Fiscal year end: '),function(x) '[['[2])
sapply(strsplit(special_notes,'Fiscal year end: '),function(x) '[['[1])
sapply(strsplit(special_notes,'Fiscal year end: '),function(x) [[x]][2])
sapply(strsplit(special_notes,'Fiscal year end: ')
)
gsub('Fiscal year end: ','',special_notes)
special_notes<-gsub('Fiscal year end: ','',special_notes)
strsplit(' ',special_notes)
strsplit(special_notes,' ')
strsplit(special_notes,' ')[1]
strsplit(special_notes,' ')[1][1]
strsplit(special_notes,' ')[[1]]
strsplit(special_notes,' ')[[1]][1]
sapply(strsplit(special_notes,' '),'[[')
sapply(strsplit(special_notes,' '),FUN = '[[')
sapply(strsplit(special_notes,' '),print)
temp<-strsplit(special_notes,' ')
sapply(temp,'[[')
sapply(temp,print)
special_notes
grep('^June',special_notes)
sum(grep('^June',special_notes))
length(grep('^June',special_notes))
library(quantmod)
amzn = getSymbols("AMZN",auto.assign=FALSE)
sampleTimes = index(amzn)
sampleTimes
substr(sampleTimes,0,4)
year<-substr(sampleTimes,0,4)
sampleTimes
class(sampleTimes)
?Date
?weekdays
weekdays(sampleTimes)
weekdays(sampleTimes)=='Monday'
year=='2012'
year[year=='2012']
weekdays(sampleTimes[year=='2012'])
weekdays(sampleTimes[year=='2012'])=='Monday'
sum(weekdays(sampleTimes[year=='2012'])=='Monday')
special_notes
strsplit(special_notes,' ')
sapply(strsplit(special_notes,' '),function(x){x[1]})
?gsub
str_t
library(stringr)
str_trim(special_notes)
data(spam)
library(krenlab)
library(kernlab)
install.packages('kernlab')
library
library
library(kernlab)
data(spam)
30*.75
70*.47
x*.75+(1-x)*.47=30
x*.75+(1-x)*.47=30
0.47-0.28*x=0.3
x<-read.csv('../DifferentialExpressionTools/Random2/profiled_A1.pro')
head(x)
x<-read.table('../DifferentialExpressionTools/Random2/profiled_A1.pro',sep='\t')
head(x)
sum(x[,6])
str(x)
sum(as.numeric(x[,6]))
read.csv('../DifferentialExpressionTools/count_table_merged.csv')
library(DESeq)
read.csv('../DifferentialExpressionTools/RealDataset/final.csv')
str(x)
x<-read.csv('../DifferentialExpressionTools/count_table_merged.csv')
mean(x[,-1])
head(x)
colMeans(x)
colMeans(x[,-1])
hist(x[,2])
hist(x[,2],40)
hist(log(x[,2]),40)
hist(log(x[,3]),40)
require(ggplot2)
states = c("alabama","arizona","arkansas","california",
"colorado","connecticut","delaware","district of columbia",
"florida","georgia","idaho","illinois",
"indiana","iowa","kansas","kentucky",
"louisiana","maine","maryland","massachusetts",
"michigan","minnesota","mississippi","missouri",
"montana","nebraska","nevada","new hampshire",
"new jersey","new mexico","new york","north carolina",
"north dakota","ohio","oklahoma","oregon",
"pennsylvania","rhode island","south carolina","south dakota",
"tennessee","texas","utah","vermont",
"virginia","washington","west virginia","wisconsin",
"wyoming")
dataset <- data.frame(region=states,val=runif(49, 0,1))
us_state_map <- map_data('state')
map_data <- merge(us_state_map, dataset, by='region', all=T)
map_data <- map_data[order(map_data$order), ]
(qplot(long, lat, data=map_data, geom="polygon", group=group, fill=val)
+ theme_bw() + labs(x="", y="", fill="")
+ scale_fill_gradient(low='#EEEEEE', high='darkgreen')
+ theme(title="I was created using gplot2!",
legend.position="bottom", legend.direction="horizontal"))
head(map_data)
rm(list=la())
rm(list=ls())
library(ggplot2)
map_data()
?map_data()
map_data('state')
map('state')
map('us')
map('usa')
map('france')
map('world')
map('world2')
map.axes()
map.cities()
data(worldMapEnv)
worldMapEnv
data(world.cities)
head(world.cities)
world.cities=='Taiwan'
world.cities[world.cities=='Taiwan']
world.cities[world.cities[,2]=='Taiwan',]
map(world2,regions = 'asia')
map(world,regions = 'asia')
map('world2',regions = 'asia')
world2map<-map_data('world2')
head(world2map)
world2map[[region]]=='Taiwan'
world2map[['region']]=='Taiwan'
world2map[world2map[['region']]=='Taiwan',]
world2map[world2map[['region']]=='China',]
head(world2map)
world2map[world2map[['region']]=='Vatican',]
world2map[world2map[['region']]=='Italy',]
world2map[world2map[['region']]=='Hong kong',]
world2map[world2map[['region']]=='China',]
world2map[world2map[['sub region']]=='Taiwan',]
taiwanmap<-world2map[world2map[['subregion']]=='Taiwan',]
ggplot(taiwanmap)<-geom_polygon()
ggplot(taiwanmap)+geom_polygon()
ggplot(taiwanmap)+geom_polygon(aes(x=long, y=lat, group = group),colour="white", fill="grey10" ))
ggplot(taiwanmap)+geom_polygon(aes(x=long, y=lat, group = group),colour="white", fill="grey10" )
read.table('/Users/NeilWu/Dropbox/Friends/小貓.txt')
read.table('/Users/NeilWu/Dropbox/Friends/小貓.txt',fill = F)
read.table('/Users/NeilWu/Dropbox/Friends/小貓.txt',sep='')
read.table('/Users/NeilWu/Dropbox/Friends/小貓.txt',sep='     TT')
read.table('/Users/NeilWu/Dropbox/Friends/小貓.txt')
0.96^25
setwd('../JohnHopkins - Datascience - Projects/RepData_PeerAssessment1/')
dir()
